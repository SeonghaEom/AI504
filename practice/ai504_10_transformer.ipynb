{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE_MQj2V-mAD"
   },
   "source": [
    "# [AI 504] Programming for AI, Fall 2021\n",
    "# Practice 10: Transformers\n",
    "----- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okJXS1-OEbN7"
   },
   "source": [
    "#### [Notifications]\n",
    "- If you have any questions, feel free to ask\n",
    "- For additional questions, send emails: dyan.lee@kaist.ac.kr    \n",
    "      \n",
    "\n",
    "     \n",
    "     \n",
    "# Table of contents\n",
    "1. [Prepare input](#1)\n",
    "2. [Implement Transformer](#2)\n",
    "3. [Train and Evaluate](#3)\n",
    "4. [Visualize attention](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2Bj-rF9EbN7"
   },
   "source": [
    "# Prepare essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nHrI30vIEbN7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.14.0\n",
      "  Downloading torchtext-0.14.0-cp39-cp39-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.13.0\n",
      "  Downloading torch-1.13.0-cp39-cp39-manylinux1_x86_64.whl (890.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 890.2 MB 13 kB/s  eta 0:00:012     |██████████████████              | 503.2 MB 9.8 MB/s eta 0:00:40     |█████████████████████████▎      | 703.3 MB 8.6 MB/s eta 0:00:22     |████████████████████████████▌   | 791.7 MB 8.6 MB/s eta 0:00:12\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from torchtext==0.14.0) (4.64.0)\n",
      "Requirement already satisfied: numpy in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from torchtext==0.14.0) (1.22.3)\n",
      "Requirement already satisfied: requests in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from torchtext==0.14.0) (2.27.1)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 44 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from torch==1.13.0->torchtext==0.14.0) (4.1.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 12 kB/s  eta 0:00:012     |███████████████████████▌        | 408.3 MB 8.8 MB/s eta 0:00:18\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (61.2.0)\n",
      "Requirement already satisfied: wheel in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (0.37.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests->torchtext==0.14.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests->torchtext==0.14.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests->torchtext==0.14.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests->torchtext==0.14.0) (1.26.9)\n",
      "Installing collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0 torchtext-0.14.0\n",
      "fatal: destination path 'attentionviz' already exists and is not an empty directory.\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from de-core-news-sm==3.4.0) (3.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.22.3)\n",
      "Requirement already satisfied: setuptools in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 269 kB/s eta 0:00:01    |█████▊                          | 2.3 MB 8.3 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.22.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: setuptools in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: jinja2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: spacy in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (0.10.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: setuptools in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/seongha/\u001b[A\u001b[B\u001b[B/envs/LTML/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "!pip install torchtext==0.14.0\n",
    "!git clone https://github.com/sjpark9503/attentionviz.git\n",
    "!python -m spacy download de\n",
    "!python -m spacy download en\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sayXpp8FEbN8"
   },
   "source": [
    "# I. Prepare input\n",
    "<a id='1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KWOpLYfEbN8"
   },
   "source": [
    "We've already learned how to preprocess the text data in week 8, 9 & 10.\n",
    "\n",
    "You can see some detailed explanation about translation datasets in [torchtext](https://pytorch.org/text/), [practice session,week 9](https://classum.com/main/course/7726/103) and [PyTorch NMT tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kaduS25kEbN8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/\u001b[A\u001b[B\u001b[B/envs/w4cNew/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            batch_first=True,\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            batch_first=True,\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE_u1Qg-EbN8"
   },
   "source": [
    "# II. Implement Transformer\n",
    "<a id='2'></a>\n",
    "In practice week 11, we will learn how to implement the __[Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Vaswani et al., 2017)__\n",
    "\n",
    "The overall architecutre is as follows:\n",
    "![picture](http://incredible.ai/assets/images/transformer-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqEVVfl-EbN8"
   },
   "source": [
    "## 1. Basic building blocks\n",
    "\n",
    "In this sections, we will implement the building blocks of the transformer: [Multi-head attention](#1a), [Position wise feedforward network](#1b) and [Positional encoding](#1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeI2oINrEbN8"
   },
   "source": [
    "### a. Attention\n",
    "<a id='1a'></a>\n",
    "In this section, you will implement scaled dot-product attention and multi-head attention.\n",
    "\n",
    "__Scaled dot product:__\n",
    "\n",
    "![picture](http://incredible.ai/assets/images/transformer-scaled-dot-product.png)\n",
    "\n",
    "__Multi-head attention:__\n",
    "\n",
    "![picture](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "Equation:\n",
    "\n",
    "$$\\begin{align} \\text{MultiHead}(Q, K, V) &= \\text{Concat}(head_1, ...., head_h) W^O \\\\\n",
    "\\text{where head}_i &= \\text{Attention} \\left( QW^Q_i, K W^K_i, VW^v_i \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "__Query, Key and Value projection:__\n",
    "\n",
    "![picture](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "07AkqQcqEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from einops import rearrange\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        num_heads,\n",
    "        dropout=0.0,\n",
    "        bias=False,\n",
    "        encoder_decoder_attention=False,  # False: encoder self-attention/ True: decoder cross attention\n",
    "        causal = False # (causal) masked attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.emb_dim, \"emb_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "        self.causal = causal\n",
    "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        \"\"\"\n",
    "        To-Do : Reshape input\n",
    "          Args : batch_size X sequence_length X embedding dimension\n",
    "          Return : batch_size X # attention head X sequence_length X head dimension\n",
    "        \"\"\"\n",
    "        new_sh = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
    "        x = x.view(*new_sh)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "        # This is equivalent to\n",
    "        # return x.transpose(1,2)\n",
    "    \n",
    "    def scaled_dot_product(self, \n",
    "                           query: torch.Tensor, \n",
    "                           key: torch.Tensor, \n",
    "                           value: torch.Tensor,\n",
    "                           attention_mask: torch.BoolTensor):\n",
    "        \"\"\"\n",
    "        To-Do : Implement scaled dot product\n",
    "          Args:\n",
    "            Query (Tensor): shape `(batch, seq_len, emb_dim)`\n",
    "            Key (Tensor): shape `(batch, seq_len, emb_dim)`\n",
    "            Value (Tensor): shape `(batch, seq_len, emb_dim)`\n",
    "            attention_mask: binary BoolTensor of shape `(batch, seq_len)` or `(seq_len, seq_len)`\n",
    "\n",
    "          Returns:\n",
    "            attn_output : attended output (result of attention mechanism)\n",
    "            attn_weights: value of each attention\n",
    "        \"\"\"\n",
    "        attn_weights = torch.matmul(query, torch.transpose(key, -1, -2)) / torch.sqrt(self.emb_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1), float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)  # softmax(QK^T/sqrt(d))\n",
    "        attn_probability = F.dropout(attn_weights, p=self.dropout, training=True)\n",
    "        attn_output = torch.matmul(attn_probability, value)\n",
    "        return attn_output, attn_weights\n",
    "    \n",
    "    def MultiHead_scaled_dot_product(self, \n",
    "                       query: torch.Tensor, \n",
    "                       key: torch.Tensor, \n",
    "                       value: torch.Tensor,\n",
    "                       attention_mask: torch.BoolTensor):\n",
    "        \"\"\"\n",
    "        To-Do : Implement Multi-head version of scaled dot product, please also take the causal masking into account.\n",
    "          Args:\n",
    "            Query (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
    "            Key (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
    "            Value (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
    "            attention_mask: binary BoolTensor of shape `(batch, src_len)` or `(seq_len, seq_len)`\n",
    "\n",
    "          Returns:\n",
    "            attn_output : attended output (result of attention mechanism)\n",
    "            attn_weights: value of each attention\n",
    "        \"\"\"\n",
    "        ##scaled dot product\n",
    "        attn_weights = torch.matmul(query, torch.transpose(key, -1, -2)) / torch.sqrt(self.emb_dim)\n",
    "\n",
    "        if attention_mask:\n",
    "          attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1), float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_probability = F.dropout(attn_weights, p=self.dropout, training=True)\n",
    "        attn_output = torch.matmul(attn_probability, value)\n",
    "\n",
    "        ## concat multiheads\n",
    "        attn_output = attn_output.permute(0,2,1,3).contiguous()\n",
    "        concat_attn_output_shape = attn_output.size()[:-2] + (self.embed_dim)\n",
    "        attn_output = attn_output.view(*concat_attn_output_shape)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        ):\n",
    "        q = self.q_proj(query)\n",
    "        # Enc-Dec attention\n",
    "        if self.encoder_decoder_attention:\n",
    "            k = self.k_proj(key)\n",
    "            v = self.v_proj(key)\n",
    "        # Self attention\n",
    "        else:\n",
    "            k = self.k_proj(query)\n",
    "            v = self.v_proj(query)\n",
    "\n",
    "        q = self.transpose_for_scores(q)\n",
    "        k = self.transpose_for_scores(k)\n",
    "        v = self.transpose_for_scores(v)\n",
    "\n",
    "        attn_output, attn_weights = self.MultiHead_scaled_dot_product(q,k,v,attention_mask)\n",
    "        return attn_output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b528gtwHEbN8"
   },
   "source": [
    "### b. Position-wise feed forward network\n",
    "<a id='1b'></a>\n",
    "In this section, we will implement position-wise feed forward network\n",
    "\n",
    "$$\\text{FFN}(x) = \\max \\left(0, x W_1 + b_1 \\right) W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sBqWWdIyEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w_1 = nn.Linear(emb_dim, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, emb_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        To-Do : Implement position-wise feed forward network\n",
    "          Args:\n",
    "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.activation(self.w_1(x))\n",
    "        x = F.dropput(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.w_2(x)\n",
    "        x = F.dropput(x, p=self.dropout, training=self.training)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9-qkoUKEbN8"
   },
   "source": [
    "### c. Sinusoidal Positional Encoding\n",
    "<a id='1c'></a>\n",
    "In this section, we will implement sinusoidal positional encoding\n",
    "\n",
    "$$\\begin{align}\n",
    "PE(pos, 2i) &= \\sin \\left( pos / 10000^{2i / d_{model}} \\right)  \\\\\n",
    "PE(pos, 2i+1) &= \\cos \\left( pos / 10000^{2i / d_{model}} \\right)  \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tsiJalEvEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
    "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
    "        super().__init__(num_positions, embedding_dim)\n",
    "        self.weight = self._init_weight(self.weight)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _init_weight(out: nn.Parameter):\n",
    "        n_pos, embed_dim = out.shape\n",
    "        pe = nn.Parameter(torch.zeros(out.shape))\n",
    "        for pos in range(n_pos):\n",
    "            for i in range(0, embed_dim, 2):\n",
    "              \"\"\"\n",
    "              To-Do : Implement sinusoidal positional encoding\n",
    "              \"\"\"\n",
    "              pe[pos, i].data.copy_(torch.tensor(np.sin(pos / 10000**(i/embed_dim))))\n",
    "              pe[pos, i+1].data.copy_(torch.tensor(np.cos(pos / 10000**(i + 1 / embed_dim))))\n",
    "        pe.detach_()\n",
    "                \n",
    "        return pe\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids):\n",
    "        bsz, seq_len = input_ids.shape[:2]\n",
    "        positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdhwI3hPEbN8"
   },
   "source": [
    "## 2. Transformer Encoder\n",
    "\n",
    "Now we have all basic building blocks which are essential to build Transformer. \n",
    "\n",
    "Let's implement Transformer step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6ym2hKzEbN8"
   },
   "source": [
    "### a. Encoder layer\n",
    "In this section, we will implement single layer of Transformer encoder.\n",
    "![picture](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6B93kjUlEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        self.self_attn = MultiHeadAttention(            \n",
    "            emb_dim=self.emb_dim,\n",
    "            num_heads=config.attention_heads, \n",
    "            dropout=config.attention_dropout)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "\n",
    "    def forward(self, x, encoder_padding_mask):\n",
    "        \"\"\"\n",
    "        To-Do : Implement transformer encoder layer\n",
    "          Args:\n",
    "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
    "            encoder_padding_mask: binary BoolTensor of shape `(batch, src_len)`\n",
    "\n",
    "          Returns:\n",
    "            x : encoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            self_attn_weights: self attention score\n",
    "        \"\"\"\n",
    "        ## residual\n",
    "        residual = x\n",
    "        ## MHA\n",
    "        x, attn_weights = self.self_attn(query=x, key=x, attention_mask=encoder_padding_mask)\n",
    "        x  = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        ## layer norm\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "\n",
    "        ## position wise FFW\n",
    "        x = self.PositionWiseFeedForward(x)\n",
    "        ## layer norm\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "\n",
    "        ## clamping , for preventing gradient explosion\n",
    "        if torch.isinf(x).any() or torch.isnan(x).any():\n",
    "          clamp_value = torch.finfo(x.dtype).max - 1000\n",
    "          x = torch.clamp(x, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LygNGzM0EbN8"
   },
   "source": [
    "### b. Encoder\n",
    "\n",
    "Stack encoder layers and build full Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nZOAlAv7EbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, embed_tokens):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        emb_dim = embed_tokens.embedding_dim\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
    "                config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
    "            )\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        To-Do : Implement the transformer encoder\n",
    "          Args:\n",
    "            input_ids (Tensor): input to the layer of shape `(batch, seq_len)`\n",
    "            attention_mask: binary BoolTensor of shape `(batch, src_len)`\n",
    "\n",
    "          Returns:\n",
    "            x: encoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            self_attn_scores: a list of self attention score of each layer\n",
    "        \"\"\"\n",
    "        ## input embeds\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        embed_pos = self.embed_positions(input_ids)\n",
    "        x = inputs_embeds + embed_pos\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        ## encoding\n",
    "        self_attn_scores = []\n",
    "        for encoder_layer in self.layers:\n",
    "          x, attn = encoder_layer(x, attention_mask)\n",
    "          self_attn_scores.append(attn.detach())\n",
    "\n",
    "        return x, self_attn_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgjqDJnKEbN8"
   },
   "source": [
    "## 3. Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73LEB0mBEbN8"
   },
   "source": [
    "### a.Decoder layer\n",
    "In this section, we will implement single layer of Transformer decoder.\n",
    "![picture](http://incredible.ai/assets/images/transformer-decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-HgMu2QCEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        ##1\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            emb_dim=self.emb_dim,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            causal=True,\n",
    "        )\n",
    "        self.dropout = config.dropout\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        ##2\n",
    "        self.encoder_attn = MultiHeadAttention(\n",
    "            emb_dim=self.emb_dim,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            encoder_decoder_attention=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "        ## 3\n",
    "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask=None,\n",
    "        causal_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        To-Do : Implement the transformer decoder layer\n",
    "          Args:\n",
    "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
    "            encoder_hidden_states: output from the encoder, used for\n",
    "                encoder-side attention\n",
    "            encoder_attention_mask: binary BoolTensor of shape `(batch, src_len)` to mask out encoder padding\n",
    "            causal_mask: binary BoolTensor of shape `(batch, src_len)` to mask out future tokens in decoder.\n",
    "\n",
    "\n",
    "          Returns:\n",
    "            x: decoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            self_attn_weights: self attention score\n",
    "            cross_attn_weights: encoder-decoder attention score\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        ## Masked causal (Self) attention \n",
    "        residual = x ## residual\n",
    "        x, self_attn_weights = self.self_attn(query=x, key=x, attention_mask=causal_mask)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = x+residual\n",
    "        self.self_attn_layer_norm(x)\n",
    "\n",
    "        ## Cross attention\n",
    "        residual = x #residual\n",
    "        x, cross_attn_weights = self.encoder_attn(query=x, key=encoder_hidden_states, attention_mask=encoder_attention_mask)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = x+residual\n",
    "        self.encoder_attn_layer_norm(x)\n",
    "\n",
    "        ## position wise FFW\n",
    "        x = self.PositionWiseFeedForward(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "        return (\n",
    "            x,\n",
    "            self_attn_weights,\n",
    "            cross_attn_weights,\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAJQ-q5mEbN8"
   },
   "source": [
    "### b. Decoder\n",
    "\n",
    "Stack decoder layers and build full Transformer decoder.\n",
    "\n",
    "Unlike the encoder, you need to do one more job: pass the causal(unidirectional) mask to the decoder self attention layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gEMa6owhEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`DecoderLayer`\n",
    "\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (torch.nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, embed_tokens: nn.Embedding):\n",
    "        super().__init__()\n",
    "        self.dropout = config.dropout\n",
    "        self.padding_idx = embed_tokens.padding_idx\n",
    "        self.max_target_positions = config.max_position_embeddings\n",
    "        self.embed_tokens = embed_tokens\n",
    "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
    "            config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
    "        )\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])  # type: List[DecoderLayer]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        encoder_hidden_states,\n",
    "        encoder_attention_mask,\n",
    "        decoder_causal_mask,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        To-Do : Implement the transformer decoder\n",
    "\n",
    "        Args:\n",
    "            input_ids (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for teacher forcing\n",
    "            encoder_hidden_states: output from the encoder, used for\n",
    "                encoder-side attention\n",
    "            encoder_attention_mask: binary BoolTensor of shape `(batch, src_len)` to mask out encoder padding\n",
    "            causal_mask: binary BoolTensor of shape `(batch, src_len)` to mask out future tokens in decoder.\n",
    "\n",
    "          Returns:\n",
    "            x: decoded output of shape `(batch, seq_len, emb_dim)`\n",
    "            cross_attn_scores: list of encoder-decoder attention score of each layer\n",
    "        \"\"\"\n",
    "        ## positional embedding\n",
    "        positions = self.embed_positions(input_ids)\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        x += positions\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # decode\n",
    "        cross_attention_scores = []\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "          x, layer_self_attn, layer_cross_attn = decoder_layer(query=x,\n",
    "              encoder_hidden_states=encoder_hidden_states,\n",
    "              encoder_attention_mask=encoder_attention_mask,\n",
    "              causal_mask=decoder_causal_mask\n",
    "          )\n",
    "          cross_attention_scores.append(layer_cross_attn.detach())\n",
    "\n",
    "        return x, cross_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr0g3oeIEbN8"
   },
   "source": [
    "## 4. Transformer\n",
    "\n",
    "Let's combine encoder and decoder in one place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M4aZzq8GEbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, SRC,TRG,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.SRC = SRC\n",
    "        self.TRG = TRG\n",
    "        \n",
    "        self.enc_embedding = nn.Embedding(len(SRC.vocab), config.emb_dim, padding_idx=SRC.vocab.stoi['<pad>'])\n",
    "        self.dec_embedding = nn.Embedding(len(TRG.vocab), config.emb_dim, padding_idx=TRG.vocab.stoi['<pad>'])\n",
    "\n",
    "        self.encoder = Encoder(config, self.enc_embedding)\n",
    "        self.decoder = Decoder(config, self.dec_embedding)\n",
    "        \n",
    "        self.prediction_head = nn.Linear(config.emb_dim,len(TRG.vocab))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def generate_mask(self,src,trg):\n",
    "        \"\"\"\n",
    "        To-Do : Generate mask for encoder and decoder attention.\n",
    "\n",
    "        Args:\n",
    "            src(LongTensor): Input to the transformer of shape (batch_size, seq_len)  \n",
    "            trg(LongTensor): Decoding target of the transformer of shape (batch_size, seq_len)  \n",
    "\n",
    "          Returns:\n",
    "            enc_attention_mask: padding mask for encoder\n",
    "            dec_attention_mask: causal mask for decoder\n",
    "        \"\"\"\n",
    "        # Mask encoder attention to ignore padding\n",
    "        enc_attention_mask = src.eq(SRC.vocab.stoi['<pad>']).to(device) # torch.Size([128, 25])\n",
    "        # Mask decoder attention for causality\n",
    "        tmp = torch.ones(trg.size(1), trg.size(1), dtype=torch.bool) # torch.Size([28, 28])\n",
    "        mask = torch.arange(tmp.size(-1)) # torch.Size([28])\n",
    "        dec_attention_mask = tmp.masked_fill_(mask < (mask + 1).view(tmp.size(-1), 1), False).to(device) # torch.Size([28, 28])\n",
    "        return enc_attention_mask, dec_attention_mask\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "                else:\n",
    "                    nn.init.constant_(param.data, 0)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        trg,\n",
    "    ):\n",
    "        enc_attention_mask, dec_causal_mask = self.generate_mask(src, trg)\n",
    "        encoder_output, encoder_attention_scores = self.encoder(\n",
    "                input_ids=src,\n",
    "                attention_mask=enc_attention_mask\n",
    "            )\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        decoder_output, decoder_attention_scores = self.decoder(\n",
    "            trg,\n",
    "            encoder_output,\n",
    "            encoder_attention_mask=enc_attention_mask,\n",
    "            decoder_causal_mask=dec_causal_mask,\n",
    "        )\n",
    "        decoder_output = self.prediction_head(decoder_output)\n",
    "\n",
    "        return decoder_output, encoder_attention_scores, decoder_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WU-llE39EbN8"
   },
   "source": [
    "# III. Train & Evaluate\n",
    "<a id='3'></a>\n",
    "This section is very similar to week 9, so please refer to it for detailed description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZRMlUmxEbN8"
   },
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easydict\n",
      "  Using cached easydict-1.10.tar.gz (6.4 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for easydict: filename=easydict-1.10-py3-none-any.whl size=6507 sha256=dd75b0d7af13ba54ec379ca48b41004cf0c1fd8bd467d9dafbb09d4d2c6dd810\n",
      "  Stored in directory: /home/seongha/.cache/pip/wheels/fe/4e/02/c9c3154e4845bfdbf1fdf344f5a89f16dcbb4f627a908c9974\n",
      "Successfully built easydict\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.10\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BlIc_VKaEbN8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/\u001b[A\u001b[B\u001b[B/envs/w4cNew/lib/python3.8/site-packages/torch/cuda/__init__.py:106: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "config = easydict.EasyDict({\n",
    "    \"emb_dim\":64,\n",
    "    \"ffn_dim\":256,\n",
    "    \"attention_heads\":4,\n",
    "    \"attention_dropout\":0.0,\n",
    "    \"dropout\":0.2,\n",
    "    \"max_position_embeddings\":512,\n",
    "    \"encoder_layers\":3,\n",
    "    \"decoder_layers\":3,\n",
    "    \n",
    "})\n",
    "\n",
    "N_EPOCHS = 100\n",
    "learning_rate = 5e-4\n",
    "CLIP = 1\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "model = Transformer(SRC,TRG,config)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "            \n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hql5wOKEbN8"
   },
   "source": [
    "## 2. Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "F1HHCxXuEbN8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/AI504/practice/ai504_10_transformer.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=64'>65</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m epoch_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(iterator)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(N_EPOCHS), total\u001b[39m=\u001b[39mN_EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=67'>68</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, criterion, CLIP)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=68'>69</a>\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=70'>71</a>\u001b[0m     \u001b[39mif\u001b[39;00m best_valid_loss \u001b[39m<\u001b[39m valid_loss:\n",
      "\u001b[1;32m/home/seongha/AI504/practice/ai504_10_transformer.ipynb Cell 33'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=18'>19</a>\u001b[0m trg \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mtrg\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=22'>23</a>\u001b[0m output, enc_attention_scores, _ \u001b[39m=\u001b[39m model(src, trg)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=24'>25</a>\u001b[0m output \u001b[39m=\u001b[39m output[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000031vscode-remote?line=25'>26</a>\u001b[0m trg \u001b[39m=\u001b[39m trg[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/\u001b[A\u001b[B\u001b[B/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/%1B%5BA%1B%5BB%1B%5BB/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/%1B%5BA%1B%5BB%1B%5BB/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/%1B%5BA%1B%5BB%1B%5BB/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/seongha/%1B%5BA%1B%5BB%1B%5BB/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/seongha/%1B%5BA%1B%5BB%1B%5BB/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/seongha/%1B%5BA%1B%5BB%1B%5BB/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/%1B%5BA%1B%5BB%1B%5BB/envs/w4cNew/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/seongha/AI504/practice/ai504_10_transformer.ipynb Cell 27'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=46'>47</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=47'>48</a>\u001b[0m     src,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=48'>49</a>\u001b[0m     trg,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=49'>50</a>\u001b[0m ):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=50'>51</a>\u001b[0m     enc_attention_mask, dec_causal_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_mask(src, trg)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=51'>52</a>\u001b[0m     encoder_output, encoder_attention_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=52'>53</a>\u001b[0m             input_ids\u001b[39m=\u001b[39msrc,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=53'>54</a>\u001b[0m             attention_mask\u001b[39m=\u001b[39menc_attention_mask\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=54'>55</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=56'>57</a>\u001b[0m     \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n",
      "\u001b[1;32m/home/seongha/AI504/practice/ai504_10_transformer.ipynb Cell 27'\u001b[0m in \u001b[0;36mTransformer.generate_mask\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=18'>19</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=19'>20</a>\u001b[0m \u001b[39mTo-Do : Generate mask for encoder and decoder attention.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=20'>21</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=27'>28</a>\u001b[0m \u001b[39m    dec_attention_mask: causal mask for decoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=28'>29</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=29'>30</a>\u001b[0m \u001b[39m# Mask encoder attention to ignore padding\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=30'>31</a>\u001b[0m enc_attention_mask \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39;49meq(SRC\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstoi[\u001b[39m'\u001b[39;49m\u001b[39m<pad>\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mto(device) \u001b[39m# torch.Size([128, 25])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=31'>32</a>\u001b[0m \u001b[39m# Mask decoder attention for causality\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.6/home/seongha/AI504/practice/ai504_10_transformer.ipynb#ch0000026vscode-remote?line=32'>33</a>\u001b[0m tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(trg\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), trg\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbool) \u001b[39m# torch.Size([28, 28])\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for idx, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, enc_attention_scores, _ = model(src, trg)\n",
    "\n",
    "        output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, attention_score, _ = model(src, trg) #turn off teacher forcing\n",
    "\n",
    "            output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), total=N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if best_valid_loss < valid_loss:\n",
    "        break\n",
    "    else:\n",
    "        best_valid_loss = valid_loss\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxyJad1WEbN8"
   },
   "source": [
    "# IV. Visualization\n",
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Eop7pGEbN8"
   },
   "source": [
    "## 1. Positional embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJKGr5JfEbN8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(15, 9))\n",
    "cax = ax.matshow(model.encoder.embed_positions.weight.data.cpu().numpy(), aspect='auto',cmap=plt.cm.YlOrRd)\n",
    "fig.colorbar(cax)\n",
    "ax.set_title('Positional Embedding Matrix', fontsize=18)\n",
    "ax.set_xlabel('Embedding Dimension', fontsize=14)\n",
    "ax.set_ylabel('Sequence Length', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrLCVOWlEbN8"
   },
   "source": [
    "## 2. Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azwmQfF-EbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from attentionviz import head_view\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "train_iterator, _, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLCz7R73EbN8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'attentionviz' in sys.path:\n",
    "  sys.path += ['attentionviz']\n",
    "!pip install regex\n",
    "\n",
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkV8XEM2EbN9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SAMPLE_IDX = 131\n",
    "\n",
    "with torch.no_grad():\n",
    "  for idx,example in enumerate(test_iterator):\n",
    "    if idx == SAMPLE_IDX:\n",
    "      sample = example\n",
    "  src = sample.src\n",
    "  trg = sample.trg\n",
    "\n",
    "  output, enc_attention_score, dec_attention_score = model(src, trg) #turn off teacher forcing\n",
    "  attention_score = {'self':enc_attention_score, 'cross':dec_attention_score}\n",
    "\n",
    "  src_tok = [SRC.vocab.itos[x] for x in src.squeeze()]\n",
    "  trg_tok = [TRG.vocab.itos[x] for x in trg.squeeze()]\n",
    "\n",
    "  call_html()\n",
    "  head_view(attention_score, src_tok, trg_tok)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ai504_10_transformer.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "98b183ca1800ea4e96dc3b796f78e5d640ebd4e2c7ed81e2a7c90e09c6123cdf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('LTML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
